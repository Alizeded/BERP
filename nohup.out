[[36m2023-08-24 04:43:06,342[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-08-24 04:43:06,344[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.reverbCrowd_datamodule.crowdNoisyDataModule          
â”‚       path_clean: /home/lucianius/Projects/blindEst/data//clean.data          
â”‚       path_raw: /home/lucianius/Projects/blindEst/data//crowdNoisy.data       
â”‚       data_dir: /home/lucianius/Projects/blindEst/data/                       
â”‚       batch_size: 6                                                           
â”‚       shuffle: true                                                           
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ denoiserModule:                                                         
â”‚         _target_: src.network.denoiser_module.denoiserModule                  
â”‚         optimizer_denoiser:                                                   
â”‚           _target_: torch.optim.Adam                                          
â”‚           _partial_: true                                                     
â”‚           lr: 0.0002                                                          
â”‚           weight_decay: 0.0                                                   
â”‚         scheduler_denoiser:                                                   
â”‚           _target_: transformers.get_cosine_schedule_with_warmup              
â”‚           _partial_: true                                                     
â”‚         lr_scheduler_cfg:                                                     
â”‚           interval: epoch                                                     
â”‚           monitor: val/loss for denoiser                                      
â”‚           frequency: 1                                                        
â”‚         net_denoiser:                                                         
â”‚           _target_: src.network.models.model.denoiser                         
â”‚           ch_in: 1                                                            
â”‚           ch_out: 1                                                           
â”‚           ch_H: 64                                                            
â”‚           max_H: 1024                                                         
â”‚           depth: 4                                                            
â”‚           kernel_size: 4                                                      
â”‚           stride: 2                                                           
â”‚           dropout_prob: 0.25                                                  
â”‚           bottleneck_mode: xformer                                            
â”‚           bottleneck_masking: false                                           
â”‚           latent-depth: 3                                                     
â”‚           latent-d_hid: 256                                                   
â”‚           num_heads: 4                                                        
â”‚           len_pos_enc: 80000                                                  
â”‚       classifierModule:                                                       
â”‚         _target_: src.network.numClassifier_module.classifierModule           
â”‚         optimizer_classifier:                                                 
â”‚           _target_: torch.optim.Adam                                          
â”‚           _partial_: true                                                     
â”‚           lr: 0.0002                                                          
â”‚           weight_decay: 0.0                                                   
â”‚         scheduler_classifier:                                                 
â”‚           _target_: transformers.get_cosine_schedule_with_warmup              
â”‚           _partial_: true                                                     
â”‚         lr_scheduler_cfg:                                                     
â”‚           interval: epoch                                                     
â”‚           monitor: val/loss for classifier                                    
â”‚           frequency: 1                                                        
â”‚         net_classifier:                                                       
â”‚           _target_: src.network.models.model.encoder_N                        
â”‚           ch_in: 1022                                                         
â”‚           ch_out: 30                                                          
â”‚           num_layers: 4                                                       
â”‚           num_heads: 4                                                        
â”‚           embed_dim: 256                                                      
â”‚           ch_scale: 2                                                         
â”‚           len_pos_enc: 80000                                                  
â”‚           dropout_prob: 0.25                                                  
â”‚           masking: false                                                      
â”‚       rirRegressorModule:                                                     
â”‚         _target_: src.network.regressor_module.rirRegressorModule             
â”‚         net_denoiser_path: /home/lucianius/Projects/blindEst/logs/train/runs/2
â”‚         net_rirRegressor:                                                     
â”‚           _target_: src.network.models.model.encoder_RIR                      
â”‚           ch_in: 1                                                            
â”‚           ch_out: 3                                                           
â”‚           num_layers_encoder: 4                                               
â”‚           num_heads: 8                                                        
â”‚           embed_dim: 512                                                      
â”‚           ch_scale: 2                                                         
â”‚           len_pos_enc: 80000                                                  
â”‚           num_layers_decoder: 2                                               
â”‚           dropout_prob: 0.25                                                  
â”‚           masking: false                                                      
â”‚         optimizer:                                                            
â”‚           _target_: torch.optim.Adam                                          
â”‚           _partial_: true                                                     
â”‚           lr: 0.0002                                                          
â”‚           weight_decay: 0.0                                                   
â”‚         scheduler:                                                            
â”‚           _target_: transformers.get_cosine_schedule_with_warmup              
â”‚           _partial_: true                                                     
â”‚         optim_cfg:                                                            
â”‚           _target_: src.network.module.optim_cfg                              
â”‚           lambda_Th: 0.1                                                      
â”‚           lambda_Tt: 0.6                                                      
â”‚           lambda_volume: 0.3                                                  
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24_
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: lightning-hydra-template                                     
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       precision: 16                                                           
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/lucianius/Projects/blindEst                             
â”‚       data_dir: /home/lucianius/Projects/blindEst/data/                       
â”‚       log_dir: /home/lucianius/Projects/blindEst/logs/                        
â”‚       output_dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24
â”‚       work_dir: /home/lucianius/Projects/blindEst                             
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 0                                                                       
[[36m2023-08-24 04:43:06,372[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.reverbCrowd_datamodule.crowdNoisyDataModule>[0m
train size 221580 val size 47480 eval size 47490
[[36m2023-08-24 04:43:06,935[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.network.denoiser_module.denoiserModule>[0m
/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net_denoiser' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net_denoiser'])`.
  rank_zero_warn(
[[36m2023-08-24 04:43:07,049[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-08-24 04:43:07,050[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-08-24 04:43:07,051[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-08-24 04:43:07,052[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-08-24 04:43:07,052[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-08-24 04:43:07,052[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-08-24 04:43:07,052[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
[[36m2023-08-24 04:43:07,053[0m][[34msrc.utils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py", line 305, in __init__
    raise ModuleNotFoundError(
ModuleNotFoundError: You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lucianius/Projects/blindEst/src/utils/utils.py", line 68, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/home/lucianius/Projects/blindEst/src/train_alpha.py", line 60, in train
    logger: List[Logger] = utils.instantiate_loggers(cfg.get("logger"))
  File "/home/lucianius/Projects/blindEst/src/utils/instantiators.py", line 54, in instantiate_loggers
    logger.append(hydra.utils.instantiate(lg_conf))
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'lightning.pytorch.loggers.wandb.WandbLogger':
ModuleNotFoundError('You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.')
full_key: logger.wandb
[[36m2023-08-24 04:43:07,054[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24_04-43-06[0m
Error executing job with overrides: ['trainer=gpu', '+trainer.precision=16', 'logger=wandb']
Traceback (most recent call last):
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py", line 305, in __init__
    raise ModuleNotFoundError(
ModuleNotFoundError: You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lucianius/Projects/blindEst/src/train_alpha.py", line 131, in <module>
    main()
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/lucianius/Projects/blindEst/src/train_alpha.py", line 119, in main
    metric_dict, _ = train(cfg)
  File "/home/lucianius/Projects/blindEst/src/utils/utils.py", line 78, in wrap
    raise ex
  File "/home/lucianius/Projects/blindEst/src/utils/utils.py", line 68, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/home/lucianius/Projects/blindEst/src/train_alpha.py", line 60, in train
    logger: List[Logger] = utils.instantiate_loggers(cfg.get("logger"))
  File "/home/lucianius/Projects/blindEst/src/utils/instantiators.py", line 54, in instantiate_loggers
    logger.append(hydra.utils.instantiate(lg_conf))
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'lightning.pytorch.loggers.wandb.WandbLogger':
ModuleNotFoundError('You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.')
full_key: logger.wandb
[[36m2023-08-24 05:10:57,797[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-08-24 05:10:57,800[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.reverbCrowd_datamodule.crowdNoisyDataModule          
â”‚       path_clean: /home/lucianius/Projects/blindEst/data//clean.data          
â”‚       path_raw: /home/lucianius/Projects/blindEst/data//crowdNoisy.data       
â”‚       data_dir: /home/lucianius/Projects/blindEst/data/                       
â”‚       batch_size: 6                                                           
â”‚       shuffle: true                                                           
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ denoiserModule:                                                         
â”‚         _target_: src.network.denoiser_module.denoiserModule                  
â”‚         optimizer_denoiser:                                                   
â”‚           _target_: torch.optim.Adam                                          
â”‚           _partial_: true                                                     
â”‚           lr: 0.0002                                                          
â”‚           weight_decay: 0.0                                                   
â”‚         scheduler_denoiser:                                                   
â”‚           _target_: transformers.get_cosine_schedule_with_warmup              
â”‚           _partial_: true                                                     
â”‚         lr_scheduler_cfg:                                                     
â”‚           interval: epoch                                                     
â”‚           monitor: val/loss for denoiser                                      
â”‚           frequency: 1                                                        
â”‚         net_denoiser:                                                         
â”‚           _target_: src.network.models.model.denoiser                         
â”‚           ch_in: 1                                                            
â”‚           ch_out: 1                                                           
â”‚           ch_H: 64                                                            
â”‚           max_H: 1024                                                         
â”‚           depth: 4                                                            
â”‚           kernel_size: 4                                                      
â”‚           stride: 2                                                           
â”‚           dropout_prob: 0.25                                                  
â”‚           bottleneck_mode: xformer                                            
â”‚           bottleneck_masking: false                                           
â”‚           latent-depth: 3                                                     
â”‚           latent-d_hid: 256                                                   
â”‚           num_heads: 4                                                        
â”‚           len_pos_enc: 80000                                                  
â”‚       classifierModule:                                                       
â”‚         _target_: src.network.numClassifier_module.classifierModule           
â”‚         optimizer_classifier:                                                 
â”‚           _target_: torch.optim.Adam                                          
â”‚           _partial_: true                                                     
â”‚           lr: 0.0002                                                          
â”‚           weight_decay: 0.0                                                   
â”‚         scheduler_classifier:                                                 
â”‚           _target_: transformers.get_cosine_schedule_with_warmup              
â”‚           _partial_: true                                                     
â”‚         lr_scheduler_cfg:                                                     
â”‚           interval: epoch                                                     
â”‚           monitor: val/loss for classifier                                    
â”‚           frequency: 1                                                        
â”‚         net_classifier:                                                       
â”‚           _target_: src.network.models.model.encoder_N                        
â”‚           ch_in: 1022                                                         
â”‚           ch_out: 30                                                          
â”‚           num_layers: 4                                                       
â”‚           num_heads: 4                                                        
â”‚           embed_dim: 256                                                      
â”‚           ch_scale: 2                                                         
â”‚           len_pos_enc: 80000                                                  
â”‚           dropout_prob: 0.25                                                  
â”‚           masking: false                                                      
â”‚       rirRegressorModule:                                                     
â”‚         _target_: src.network.regressor_module.rirRegressorModule             
â”‚         net_denoiser_path: /home/lucianius/Projects/blindEst/logs/train/runs/2
â”‚         net_rirRegressor:                                                     
â”‚           _target_: src.network.models.model.encoder_RIR                      
â”‚           ch_in: 1                                                            
â”‚           ch_out: 3                                                           
â”‚           num_layers_encoder: 4                                               
â”‚           num_heads: 8                                                        
â”‚           embed_dim: 512                                                      
â”‚           ch_scale: 2                                                         
â”‚           len_pos_enc: 80000                                                  
â”‚           num_layers_decoder: 2                                               
â”‚           dropout_prob: 0.25                                                  
â”‚           masking: false                                                      
â”‚         optimizer:                                                            
â”‚           _target_: torch.optim.Adam                                          
â”‚           _partial_: true                                                     
â”‚           lr: 0.0002                                                          
â”‚           weight_decay: 0.0                                                   
â”‚         scheduler:                                                            
â”‚           _target_: transformers.get_cosine_schedule_with_warmup              
â”‚           _partial_: true                                                     
â”‚         optim_cfg:                                                            
â”‚           _target_: src.network.module.optim_cfg                              
â”‚           lambda_Th: 0.1                                                      
â”‚           lambda_Tt: 0.6                                                      
â”‚           lambda_volume: 0.3                                                  
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24_
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: lightning-hydra-template                                     
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       precision: 16                                                           
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/lucianius/Projects/blindEst                             
â”‚       data_dir: /home/lucianius/Projects/blindEst/data/                       
â”‚       log_dir: /home/lucianius/Projects/blindEst/logs/                        
â”‚       output_dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24
â”‚       work_dir: /home/lucianius/Projects/blindEst                             
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 0                                                                       
[[36m2023-08-24 05:10:57,827[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.reverbCrowd_datamodule.crowdNoisyDataModule>[0m
train size 221580 val size 47480 eval size 47490
[[36m2023-08-24 05:10:58,322[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.network.denoiser_module.denoiserModule>[0m
/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net_denoiser' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net_denoiser'])`.
  rank_zero_warn(
[[36m2023-08-24 05:10:58,528[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-08-24 05:10:58,528[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-08-24 05:10:58,529[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-08-24 05:10:58,530[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-08-24 05:10:58,530[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-08-24 05:10:58,530[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-08-24 05:10:58,530[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: lucianius (lucx). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24_05-10-57/wandb/run-20230824_051059-rzfofpo0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-glitter-4
wandb: â­ï¸ View project at https://wandb.ai/lucx/lightning-hydra-template
wandb: ğŸš€ View run at https://wandb.ai/lucx/lightning-hydra-template/runs/rzfofpo0
[[36m2023-08-24 05:11:01,646[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/lightning/fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-08-24 05:11:01,683[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-08-24 05:11:01,684[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loading `train_dataloader` to estimate number of stepping batches.
/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
[[36m2023-08-24 05:11:02,260[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2023-08-24 05:11:02,260[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/lucianius/miniconda3/envs/torch-cuda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:42: UserWarning: Encounted `nan` values in tensor. Will be removed.
  warnings.warn(*args, **kwargs)  # noqa: B028
Testing â”â•¸                                 419/7915 0:00:45 â€¢ 0:13:32 9.24it/s 
[[36m2023-08-24 05:11:48,916[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: None[0m
[[36m2023-08-24 05:11:48,917[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/lucianius/Projects/blindEst/logs/train/runs/2023-08-24_05-10-57[0m
[[36m2023-08-24 05:11:48,917[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: ERROR Control-C detected -- Run data was not synced
